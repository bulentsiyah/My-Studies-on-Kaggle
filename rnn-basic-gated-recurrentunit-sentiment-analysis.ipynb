{"cells":[{"metadata":{"_uuid":"4e8fb70b105c274ed14b46b17c8fb094b5c05ed5"},"cell_type":"markdown","source":"\n\n[Hakan CEBECİ'nin](http://https//www.udemy.com/user/software-development/)\n\n* [Doğal Dil İşleme A-Z™: (NLP)](http://https//www.udemy.com/dogal-dil-isleme/)\n\nkursundan öğrendiklerimi denediğim ve derlediğim kernelimdir.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n#pandas data setı okumaya ve duzenlemeye yardımcı olur\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, GRU, Embedding, CuDNNGRU\nfrom tensorflow.python.keras.optimizers import Adam\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/hepsiburada.csv')\ndataset.head()\n#dataframe olrak yuklendı","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e234f631281947d6063e53acaf722ddafe60a090"},"cell_type":"code","source":"target = dataset['Rating'].values.tolist()\ndata = dataset['Review'].values.tolist()\ncutoff = int(len(data) * 0.80)\nx_train, x_test = data[:cutoff], data[cutoff:]\ny_train, y_test = target[:cutoff], target[cutoff:]\nprint(x_train[100])\nprint(y_train[100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d6401befc746994f815f6d7789343ebe3a61c82"},"cell_type":"code","source":"num_words = 10000\ntokenizer = Tokenizer(num_words=num_words)\n#en sık gecen 10bın kelıme kelıme haznesınde\ntokenizer.fit_on_texts(data)\n#tokenlestırme yapıldı\n#tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da91d84ecebaa06e66acc92e8be9bda2b03567b7"},"cell_type":"code","source":"x_train_tokens = tokenizer.texts_to_sequences(x_train)\n#data ıcın yapılan durumun aynısı eğitim verisi içinde yapılıyor\nprint(x_train[800])\n#secılen cumle asagıda\nprint(x_train_tokens[800])\n#secılen cumlenın token halı asagıda","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a687996f54f01a1a186d95aafab6e61ba6062cb0"},"cell_type":"code","source":"x_test_tokens = tokenizer.texts_to_sequences(x_test)\n#test ıcındekı cumleler tokenlestırılıyor\nnum_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\nnum_tokens = np.array(num_tokens)\n# rnn belırlı boyutta verı verılmelı bu yuzden tum verılerı aynı boyuta getırmelıyız\n# np array amac daha kolay ıslem yapmak\nnp.mean(num_tokens)\n#ortalama 20 token var her cumlede","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cca577d69cda22f9abc1f6a647d2a1b1500a52a6"},"cell_type":"code","source":"print(np.max(num_tokens))\n# en con 195 kelıme\nprint(np.argmax(num_tokens))\n#en uzun olan 295 sırasını bulmak ıcın\nprint(x_train[21941])\n#en uzuna bakalım","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c34d787ce7fc9b5101189b5ae94d6b0c3f80d613"},"cell_type":"code","source":"max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\nmax_tokens = int(max_tokens)\nprint(max_tokens) #burada mean ve standart sapma degerlerıyle kac kelıme alsak daha ıyı olur onu secıyoruz\nprint(np.sum(num_tokens < max_tokens) / len(num_tokens))\n#kac tane cumle 59 kaplıyor yaklasık yuzde 95","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8999d864f75bd25f1633e0768cd9b66e35a7975f"},"cell_type":"code","source":"x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens)\n#traın ıcın 59 dan sonrası sılıyorz\nprint(x_train_pad.shape)\nx_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens)\n#test ıcın 59 dan sonrası sılıyorz\nprint(x_test_pad.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"223461fd0b13536bfb80215bb17d30f7b394da02"},"cell_type":"code","source":"print(np.array(x_train_tokens[800]))\nprint(x_train_pad[800])\n# burada paddıng eklenmısler var","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aec1d21a554e2e4bce82273ffea28e2dd2998b9b"},"cell_type":"code","source":"idx = tokenizer.word_index\n#kelımeler ve buna karsılık gelen degerler burada\ninverse_map = dict(zip(idx.values(), idx.keys()))\n#bu işlem ile hangı kelıme hangı token aldı bunu hesaplıyoruz\n# cunku keras kelıme token yapıyo ama gerı cozemıyoruz bu yuzden onemlı\n\ndef tokens_to_string(tokens):\n    words = [inverse_map[token] for token in tokens if token!=0]\n    text = ' '.join(words)\n    return text\n#buraya hangı token versek text halınde gerı donucek","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61821888e87f8d0b5efd5897f2b1e363fb13e805"},"cell_type":"code","source":"print(x_train[800])\n#bu orjınal 800 ındextekı cumle\nprint(tokens_to_string(x_train_tokens[800]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e62bb1114bee4fbd22aa88f17deea367d44aa6a"},"cell_type":"code","source":"model = Sequential()\nembedding_size = 50\n#her keılmeye karsılık gelen 50 uzunlugunda vektor var kelıme degerlının tutuldugu\nmodel.add(Embedding(input_dim=num_words,\n                    output_dim=embedding_size,\n                    input_length=max_tokens,\n                    name='embedding_layer'))\n#ınput belırlenen maksımum deger kadar\n#cıkıs 50 vektor \n#gırıs ıse verıler kelımeler (10bın uzunlugunda verı olucak)\nmodel.add(GRU(units=16, return_sequences=True))\n#return true olma sebebı tum cıkısların dıger aglara aktarılması, eger false olsaysı sadece son output cıkcaktı\n#eger gpu olurda CuDNNGRU kullanılabılır, sadece nvdıa nın kartında calısyo ve sadece hız farkı var\nmodel.add(GRU(units=8, return_sequences=True))\nmodel.add(GRU(units=4)) #default false o yuzden yazmaya gerek yok\nmodel.add(Dense(1, activation='sigmoid'))\noptimizer = Adam(lr=1e-3)\nmodel.compile(loss='binary_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e50bc5f079f8c40e85a08eb2dd7758cc6c06dbb"},"cell_type":"code","source":"model.fit(x_train_pad, y_train, epochs=5, batch_size=256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d47edf7af720b48bbde0cd591e4e63cc2488efd0"},"cell_type":"code","source":"result = model.evaluate(x_test_pad, y_test)\nprint(result[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f1c594673837659450581c03adf0f89fe6bd7ab"},"cell_type":"code","source":"y_pred = model.predict(x=x_test_pad[0:1000])\n#y pred normalde sutun halınde\ny_pred = y_pred.T[0] #burada transpozu alınarak yatay hale getırılıyor\ncls_pred = np.array([1.0 if p>0.5 else 0.0 for p in y_pred])\ncls_true = np.array(y_test[0:1000])\nincorrect = np.where(cls_pred != cls_true)\nincorrect = incorrect[0] #tuple ıceısınde o yuzden [0]\nprint(incorrect) #yanlıs tahmınler\nprint(len(incorrect))  # yanlıs tahmın sayısı\nidx = incorrect[3] #ılk yanlıs tahmın ındexi\nprint(idx)\ntext = x_test[idx]\nprint(text)\nprint(\"y_pred\",y_pred[idx])\nprint(\"cls_pred\",cls_pred[idx])\nprint(\"cls_true\",cls_true[idx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94b24d268df1a1fc85e37d9116e4c51e1f92f56a"},"cell_type":"code","source":"text1 = \"bu ürün çok iyi herkese tavsiye ederim\"\ntext2 = \"kargo çok hızlı aynı gün elime geçti\"\ntext3 = \"büyük bir hayal kırıklığı yaşadım bu ürün bu markaya yakışmamış\"\ntext4 = \"mükemmel\"\ntext5 = \"tasarımı harika ancak kargo çok geç geldi ve ürün açılmıştı tavsiye etmem\"\ntext6 = \"hiç resimde gösterildiği gibi değil\"\ntext7 = \"kötü yorumlar gözümü korkutmuştu ancak hiçbir sorun yaşamadım teşekkürler\"\ntext8 = \"hiç bu kadar kötü bir satıcıya denk gelmemiştim ürünü geri iade ediyorum\"\ntext9 = \"tam bir fiyat performans ürünü\"\ntext10 = \"beklediğim gibi çıkmadı\"\ntexts = [text1, text2, text3, text4, text5, text6, text7, text8, text9, text10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af872135218d9c7e3d21e90a9dcb178177f45806"},"cell_type":"code","source":"tokens = tokenizer.texts_to_sequences(texts)\ntokens_pad = pad_sequences(tokens, maxlen=max_tokens)\nprint(tokens_pad.shape)\ntext_pred=model.predict(tokens_pad)\nprint(text_pred)\ntext_pred = np.array([1.0 if p>0.5 else 0.0 for p in text_pred])\nprint(text_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}